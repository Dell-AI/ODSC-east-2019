{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we demonstrate how to build a neural network to classify images. The neural network will be built using the Keras style API available in Analytics Zoo, and use a BigDL optimizer to train the model.\n",
    "\n",
    "Data: \n",
    "* The dataset used is MNIST, which is a collection of handwritten numbers ranging from 0 to 9. The data training dataset contains 60000 images and the test set contains 10000 images. We will try to predict the number in each image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2018 Analytics Zoo Authors.\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message = \"numpy.dtype size changed\")\n",
    "\n",
    "import tensorflow as tf\n",
    "from zoo import init_nncontext\n",
    "from zoo.pipeline.api.net import TFOptimizer, TFDataset, TFPredictor\n",
    "from bigdl.optim.optimizer import *\n",
    "import sys\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "from bigdl.dataset import mnist\n",
    "from bigdl.dataset.transformer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Initilaize NN context, it will get a SparkContext with optimized configuration for BigDL performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = init_nncontext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create function to import and format the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data, pre-process and create TFDataset\n",
    "def get_data_rdd(dataset):\n",
    "    (images_data, labels_data) = mnist.read_data_sets(\"/tmp/mnist\", dataset)\n",
    "    # image_rdd = sc.parallelize(images_data[:data_num])\n",
    "    image_rdd = sc.parallelize(images_data)\n",
    "    # labels_rdd = sc.parallelize(labels_data[:data_num])\n",
    "    labels_rdd = sc.parallelize(labels_data)\n",
    "    rdd = image_rdd.zip(labels_rdd) \\\n",
    "        .map(lambda rec_tuple: [normalizer(rec_tuple[0], mnist.TRAIN_MEAN, mnist.TRAIN_STD),\n",
    "                                np.array(rec_tuple[1])])\n",
    "    return rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Download and read the MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_rdd = get_data_rdd(\"train\")\n",
    "testing_rdd = get_data_rdd(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Format the RDDs into a TensorFlow Dataset object. Format the shape of the input features to that of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size can be adjusted to improve the model, but must be a multiple of the total number of cores in the Spark environment\n",
    "batch_size = 512\n",
    "\n",
    "# Images are 28 by 28 pixels with one color channel for black and white; labels have no dimmensions because they are integers\n",
    "dataset = TFDataset.from_rdd(training_rdd,\n",
    "                             names=[\"features\", \"labels\"],\n",
    "                             shapes=[[28, 28, 1], []], \n",
    "                             types=[tf.float32, tf.int32],\n",
    "                             batch_size=batch_size,\n",
    "                             val_rdd=testing_rdd\n",
    "                             )\n",
    "pred_dataset = TFDataset.from_rdd(training_rdd,\n",
    "                                  names=[\"features\", \"labels\"],\n",
    "                                  shapes=[[28, 28, 1], []], \n",
    "                                  types=[tf.float32, tf.int32],\n",
    "                                  batch_per_thread=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create the model structure, with inputs being the shape of the images and outputs being the number of image classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Input(shape=[28, 28, 1]) # Must match the dimmensions of the images specified in the TFDataset\n",
    "# x = Convolution2D(6, 5, 5, activation='tanh', name='conv1_5x5')(data)\n",
    "# x = MaxPooling2D()(x)\n",
    "x = Flatten()(data)\n",
    "x = Dense(64, activation='relu')(x) # The number of dense units can be adjusted to attempt to improve the model.\n",
    "x = Dense(64, activation='relu')(x)\n",
    " # The number of dense units in the predictions (output) layer should match the number of classes\n",
    "predictions = Dense(10, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Initialize the model and the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=data, outputs=predictions)\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "optimizer = TFOptimizer.from_keras(keras_model=model, dataset=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Set location for model training summary output.\n",
    "* Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.set_train_summary(TrainSummary(\"/tmp/mnist_log\", \"mnist\"))\n",
    "optimizer.set_val_summary(ValidationSummary(\"/tmp/mnist_log\", \"mnist\"))\n",
    "# kick off training\n",
    "max_epoch = 5\n",
    "optimizer.optimize(end_trigger=MaxEpoch(max_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a TFPredictor object based on the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictor = TFPredictor.from_keras(model, pred_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Predict the classes of the data in the testing data (pred_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_rdd_preds = model_predictor.predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
