{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we demonstrate how to build a neural network to classify images. The neural network will be built using the Keras style API available in Analytics Zoo, and use a BigDL optimizer to train the model.\n",
    "\n",
    "Data: \n",
    "* The dataset used is MNIST, which is a collection of handwritten numbers ranging from 0 to 9. The data training dataset contains 60000 images and the test set contains 10000 images. We will try to predict the number in each image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2018 Analytics Zoo Authors.\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message = \"numpy.dtype size changed\")\n",
    "\n",
    "import tensorflow as tf\n",
    "from zoo import init_nncontext\n",
    "from zoo.pipeline.api.net import TFOptimizer, TFDataset, TFPredictor\n",
    "from bigdl.optim.optimizer import *\n",
    "import sys\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "from bigdl.dataset import mnist\n",
    "from bigdl.dataset.transformer import *\n",
    "\n",
    "import mnist_utilities_workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Initilaize NN context, it will get a SparkContext with optimized configuration for BigDL performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = init_nncontext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create function to import and format the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data, pre-process and create TFDataset\n",
    "def get_data_rdd(dataset):\n",
    "    (images_data, labels_data) = mnist_utilities_workshop.ws_read_data_sets(\"/opt/work/ODSC-east-2019/datasets/mnist/\", dataset)\n",
    "    # image_rdd = sc.parallelize(images_data[:data_num])\n",
    "    image_rdd = sc.parallelize(images_data)\n",
    "    # labels_rdd = sc.parallelize(labels_data[:data_num])\n",
    "    labels_rdd = sc.parallelize(labels_data)\n",
    "    rdd = image_rdd.zip(labels_rdd) \\\n",
    "        .map(lambda rec_tuple: [normalizer(rec_tuple[0], mnist.TRAIN_MEAN, mnist.TRAIN_STD),\n",
    "                                np.array(rec_tuple[1])])\n",
    "    return rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Download and read the MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Extracting', '/home/bala/odsc/ODSC-east-2019/datasets/mnist/train-images-idx3-ubyte.gz')\n",
      "('Extracting', '/home/bala/odsc/ODSC-east-2019/datasets/mnist/train-labels-idx1-ubyte.gz')\n",
      "('Extracting', '/home/bala/odsc/ODSC-east-2019/datasets/mnist/t10k-images-idx3-ubyte.gz')\n",
      "('Extracting', '/home/bala/odsc/ODSC-east-2019/datasets/mnist/t10k-labels-idx1-ubyte.gz')\n"
     ]
    }
   ],
   "source": [
    "training_rdd = get_data_rdd(\"train\")\n",
    "testing_rdd = get_data_rdd(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Format the RDDs into a TensorFlow Dataset object. Format the shape of the input features to that of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size can be adjusted to improve the model, but must be a multiple of the total number of cores in the Spark environment\n",
    "batch_size = 512\n",
    "\n",
    "# Images are 28 by 28 pixels with one color channel for black and white; labels have no dimmensions because they are integers\n",
    "dataset = TFDataset.from_rdd(training_rdd,\n",
    "                             names=[\"features\", \"labels\"],\n",
    "                             shapes=[[28, 28, 1], []], \n",
    "                             types=[tf.float32, tf.int32],\n",
    "                             batch_size=batch_size,\n",
    "                             val_rdd=testing_rdd\n",
    "                             )\n",
    "pred_dataset = TFDataset.from_rdd(training_rdd,\n",
    "                                  names=[\"features\", \"labels\"],\n",
    "                                  shapes=[[28, 28, 1], []], \n",
    "                                  types=[tf.float32, tf.int32],\n",
    "                                  batch_per_thread=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create the model structure, with inputs being the shape of the images and outputs being the number of image classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Input(shape=[28, 28, 1]) # Must match the dimmensions of the images specified in the TFDataset\n",
    "# x = Convolution2D(6, 5, 5, activation='tanh', name='conv1_5x5')(data)\n",
    "# x = MaxPooling2D()(x)\n",
    "x = Flatten()(data)\n",
    "x = Dense(64, activation='relu')(x) # The number of dense units can be adjusted to attempt to improve the model.\n",
    "x = Dense(64, activation='relu')(x)\n",
    " # The number of dense units in the predictions (output) layer should match the number of classes\n",
    "predictions = Dense(10, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the model and the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createRMSprop\n",
      "creating: createClassNLLCriterion\n",
      "creating: createLoss\n",
      "creating: createZooKerasAccuracy\n",
      "INFO:tensorflow:Froze 6 variables.\n",
      "INFO:tensorflow:Converted 6 variables to const ops.\n",
      "creating: createTFTrainingHelper\n",
      "creating: createTFValidationMethod\n",
      "creating: createTFValidationMethod\n",
      "creating: createIdentityCriterion\n",
      "creating: createMaxEpoch\n",
      "creating: createDistriOptimizer\n",
      "creating: createEveryEpoch\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=data, outputs=predictions)\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "optimizer = TFOptimizer.from_keras(keras_model=model, dataset=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set location for model training summary output.\n",
    "## Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createTrainSummary\n",
      "creating: createValidationSummary\n",
      "creating: createMaxEpoch\n"
     ]
    }
   ],
   "source": [
    "optimizer.set_train_summary(TrainSummary(\"/tmp/mnist_log\", \"mnist\"))\n",
    "optimizer.set_val_summary(ValidationSummary(\"/tmp/mnist_log\", \"mnist\"))\n",
    "max_epoch = 15\n",
    "optimizer.optimize(end_trigger=MaxEpoch(max_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the weights so it can be read later for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"mnist_keras.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Tensorboard\n",
    "\n",
    "tensorboard --logdir=/tmp/mnist_log/mnist/train --port 8082"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
